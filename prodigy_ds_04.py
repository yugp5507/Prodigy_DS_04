# -*- coding: utf-8 -*-
"""Prodigy_DS_04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17lpujBo5J99hiIQ5TBxF586UtTOytc34
"""

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist
import string
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk

# Download the VADER lexicon
nltk.download('vader_lexicon')

Reviewdata = pd.DataFrame(data={'tweet_ID': [], 'Entity': [], 'Sentiment': [], 'Tweet_content': []})
Reviewdata.columns = ['tweet_ID', 'Entity', 'Sentiment', 'Tweet_content']

colnames=Reviewdata.columns = ['tweet_ID', 'Entity', 'Sentiment', 'Tweet_content']
Reviewdata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Internship/twitter_training.csv', names=colnames)
Reviewdata.head(10)

print(Reviewdata.shape)
print(Reviewdata.info())

colnames=Reviewdata.columns = ['tweet_ID', 'Entity', 'Sentiment', 'Tweet_content']
Reviewdata = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Data/Internship/twitter_training.csv",names=colnames)
Reviewdata.head(10)

print(Reviewdata.shape)
print(Reviewdata.info())

Reviewdata.isnull().sum()

Reviewdata.dropna(inplace=True)

Reviewdata.isnull().sum()

# Checking missing values in the dataset and calculating the percentage for each column
count = Reviewdata.isnull().sum().sort_values(ascending=False)
percentage = ((Reviewdata.isnull().sum() / len(Reviewdata)) * 100).sort_values(ascending=False)

# Creating a DataFrame to display count and percentage of missing values
missing_data = pd.concat([count, percentage], axis=1, keys=['Count', 'Percentage'])

# Printing count and percentage of missing values
print('Count and percentage of missing values for the columns:')
print(missing_data)

positive_tweets = Reviewdata[Reviewdata['Sentiment'] == 'Positive']
negative_tweets = Reviewdata[Reviewdata['Sentiment'] == 'Negative']
neutral_tweets = Reviewdata[Reviewdata['Sentiment'] == 'Neutral']

# Correcting the counts using the lengths of the filtered tweets
positive_count = len(positive_tweets)
negative_count = len(negative_tweets)
neutral_count = len(neutral_tweets)

labels = ['Positive', 'Negative', 'Neutral']
sizes = [positive_count, negative_count, neutral_count]
colors = ['#FF9999', '#66B2FF', '#99FF99']

explode = (0.1, 0, 0)

plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.axis('equal')

plt.title('Sentiment Distribution in Tweets')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

# Displaying the percentage distribution of sentiment categories
print('Percentage for default\n')
print(round(Reviewdata.Sentiment.value_counts(normalize=True) * 100, 2))

# Creating a bar chart with a different color
round(Reviewdata.Sentiment.value_counts(normalize=True) * 100, 2).plot(kind='bar', color = ['royalblue', 'lightsteelblue', 'skyblue'])
plt.title('Percentage Distributions by review type')
plt.xlabel('')
plt.show()

nltk.download('punkt')
nltk.download('stopwords')

# Combining all tweets into single string
all_tweets = ' '.join(Reviewdata['Tweet_content'].dropna())

# Tokenization and preprocessing
stop_words = set(stopwords.words('english'))
tokens = word_tokenize(all_tweets.lower())
tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

# Generating frequency distribution
freq_dist = FreqDist(tokens)

# Generating Word Cloud
wordcloud = WordCloud(width=1600, height=800, background_color='white').generate_from_frequencies(freq_dist)

# Plotting the Word Cloud image
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Assumming you have already loaded your data into the Reviewdata DataFrame

tweets = Reviewdata['Tweet_content'].dropna()

# Initialize\ing SentimentIntensityAnalyzer from nltk
sia = SentimentIntensityAnalyzer()

# Categorizing tweets into positive, negative, and neutral
positive_tweets = [tweet for tweet in tweets if sia.polarity_scores(tweet)['compound'] > 0.1]
negative_tweets = [tweet for tweet in tweets if sia.polarity_scores(tweet)['compound'] < -0.1]
neutral_tweets = [tweet for tweet in tweets if -0.1 <= sia.polarity_scores(tweet)['compound'] <= 0.1]

# Generating and plotting word cloud with customizable size
def generate_wordcloud(text, title, width=800, height=400):
    wordcloud = WordCloud(width=width, height=height, background_color='white').generate(' '.join(text))
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')  # Turn off the axis
    plt.title(title)
    plt.show()

# Generating and plotting larger word clouds for each sentiment
generate_wordcloud(positive_tweets, 'Positive Sentiment Word Cloud', width=1600, height=800)
generate_wordcloud(negative_tweets, 'Negative Sentiment Word Cloud', width=1600, height=800)
generate_wordcloud(neutral_tweets, 'Neutral Sentiment Word Cloud', width=1600, height=800)

Reviewdata.drop(columns=['tweet_ID','Entity'],inplace=True)
Reviewdata.head(10)

#Applying first level cleanning
import re
import string

def text_clean_1(text):
    text = text.lower()
    text = re.sub('\[.*?]','',text)
    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)
    text = re.sub('\w*\d\w*','',text)
    return text

cleaned1 = lambda x: text_clean_1(x)

Reviewdata['cleaned_description'] = pd.DataFrame(Reviewdata['Tweet_content'].apply(cleaned1))
Reviewdata.head(10)

#Applying second round of cleanning
def text_clean_2(text):
    text = re.sub('[''""...]','',text)
    text = re.sub('\n','',text)
    return text
cleaned2 = lambda x: text_clean_2(x)

Reviewdata['cleaned_description_new'] = pd.DataFrame(Reviewdata['cleaned_description'].apply(cleaned2))
Reviewdata.head()

# Model Training
from sklearn.model_selection import train_test_split

Independent_var = Reviewdata.cleaned_description_new
Dependent_var = Reviewdata.Sentiment

IV_train, IV_test, DV_train, DV_test = train_test_split(Independent_var, Dependent_var, test_size=0.1, random_state=225)

print('IV_train:', len(IV_train))
print('IV_test:', len(IV_test))  # Corrected the variable name in the print statement
print('DV_train:', len(DV_train))
print('DV_test:', len(DV_test))  # Corrected the variable name in the print statement

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Sampling data (replace this with actual training data)
IV_train_sample = ["This is a positive review.", "This is a negative review.", "Another positive example."]
DV_train_sample = ["Positive", "Negative", "Positive"]

# Creating a TF-IDF vectorizer and logistic regression classifier pipeline
model_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', LogisticRegression(solver='lbfgs'))
])

# Fitting the pipeline on the training data
model_pipeline.fit(IV_train_sample, DV_train_sample)

# Now we can use the trained model for predictions

predictions = model_pipeline.predict(IV_test)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix  # Import the confusion_matrix function

# Define your TF-IDF vectorizer and logistic regression classifier
tvec = TfidfVectorizer()
clf2 = LogisticRegression(solver='lbfgs')

# Create a pipeline with TF-IDF vectorizer and logistic regression classifier
model = Pipeline([('vectorizer', tvec), ('classifier', clf2)])

# Assuming IV_train and DV_train are your training data
# Assuming IV_test and DV_test are your testing data
# Fit the model on the training data
model.fit(IV_train, DV_train)

# Make predictions on the test data
predictions = model.predict(IV_test)

# Compute and print the confusion matrix
conf_matrix = confusion_matrix(predictions, DV_test)
print(conf_matrix)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combinning all tweets into a single string
text_data = " ".join(Reviewdata['Tweet_content'].astype(str))

# Creating a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

# Plotting the Word Cloud image
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')  # Turn off the axis
plt.title('Word Cloud Based on Model Features')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score

# Calculate and print accuracy, precision, and recall
print("Accuracy:", accuracy_score(predictions, DV_test))
print("Precision:", precision_score(predictions, DV_test, average='weighted'))
print("Recall:", recall_score(predictions, DV_test, average='weighted'))

example = ['Nvidia doesn’t intend to give its opinion']
result = model_pipeline.predict(example)
print(result)

example = ["This is all based on last quarter's earnings."]
result = model.predict(example)
print(result)

example = ["This is all based on last quarter's earnings."]
result = model_pipeline.predict(example)
print(result)

example = ["you are a Simp"]
result = model_pipeline.predict(example)
print(result)

example = ['Wonderful summer photos ' ]
result = model.predict(example)
print(result)

example = ['Nvidia doesn’t intend to give its opinion ']
result = model.predict(example)
print(result)

example = ["borderland game is amazing"]
result = model.predict(example)
print(result)

